{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mcunet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmcunet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcunet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_zoo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m net_id_list, build_model, download_tflite\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mcunet'"
     ]
    }
   ],
   "source": [
    "from data_preparation import *\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from optimizers import *\n",
    "from tools import *\n",
    "from mcunet.mcunet.model_zoo import net_id_list, build_model, download_tflite\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from time import time\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/data/6_Flex_BMIS/flex_bmis/mat_data'\n",
    "subject_number = 1\n",
    "session_number = 2\n",
    "\n",
    "fs = 250\n",
    "\n",
    "\n",
    "\n",
    "record_time = 5\n",
    "train_repetition = [1, 2, 3, 4, ]\n",
    "test_repetition = [2, 5]\n",
    "\n",
    "gesture = [1, 2, 3, 4, 5, 6, 7]\n",
    "selected_gesture = [1, 2, 3, 4, 5, 6, 7]\n",
    "num_gesture = len(gesture)\n",
    "number_gestures = 7\n",
    "batch_size = 32\n",
    "\n",
    "no_channel = 7\n",
    "low_cut = 10.0\n",
    "high_cut = 99.0\n",
    "notch_freq = 60.0\n",
    "window_time = 200\n",
    "overlap = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "emg_prep = EMGDataPreparation(base_path=path, fs=fs, rec_time=record_time)\n",
    "subject_path, train_gesture, test_gesture = emg_prep.get_per_subject_file(subject_number=subject_number, num_gesture=num_gesture, train_repetition=train_repetition, test_repetition=test_repetition)\n",
    "train_data, test_data = emg_prep.load_data_per_subject(subject_path, selected_gesture=selected_gesture, train_gesture=train_gesture, test_gesture=test_gesture)\n",
    "train_data, train_labels = emg_prep.get_data_labels(train_data)\n",
    "test_data, test_labels = emg_prep.get_data_labels(test_data)\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = EMGPreprocessing(fs=fs, notch_freq=notch_freq, low_cut=low_cut, high_cut=high_cut, order=5)\n",
    "train_data = preprocess.remove_mains(train_data)\n",
    "test_data = preprocess.remove_mains(test_data)\n",
    "train_data = preprocess.bandpass_filter(train_data)\n",
    "test_data = preprocess.bandpass_filter(test_data)\n",
    "\n",
    "# Overlap Windowing\n",
    "window_train_data, window_train_labels = emg_prep.window_with_overlap(train_data, train_labels, window_time=window_time, overlap=overlap, no_channel=no_channel)\n",
    "window_test_data, window_test_labels = emg_prep.window_with_overlap(test_data, test_labels, window_time=window_time, overlap=overlap, no_channel=no_channel)\n",
    "\n",
    "# Expand Dimension\n",
    "window_train_data = np.expand_dims(window_train_data, axis=1)\n",
    "window_test_data = np.expand_dims(window_test_data, axis=1)\n",
    "\n",
    "# Apply balancing to train and test sets\n",
    "window_train_data, window_train_labels = balance_classes(window_train_data, window_train_labels)\n",
    "window_test_data, window_test_labels = balance_classes(window_test_data, window_test_labels)\n",
    "\n",
    "# Shuffle the balanced datasets\n",
    "window_train_data, window_train_labels = shuffle_data(window_train_data, window_train_labels)\n",
    "window_test_data, window_test_labels = shuffle_data(window_test_data, window_test_labels)\n",
    "\n",
    "# # Convert to Tensor\n",
    "train_dataset = EMGDataset(window_train_data, window_train_labels)\n",
    "test_dataset = EMGDataset(window_test_data, window_test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "window_train_data.shape, window_train_labels.shape, window_test_data.shape, window_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(window_train_labels, return_counts=True), np.unique(window_test_labels, return_counts=True)\n",
    "counts_train = np.unique(window_train_labels, return_counts=True)\n",
    "counts_test = np.unique(window_test_labels, return_counts=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].bar(counts_train[0], counts_train[1])\n",
    "ax[0].set_title('Training Set Distribution')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "ax[1].bar(counts_test[0], counts_test[1])\n",
    "ax[1].set_title('Test Set Distribution')\n",
    "ax[1].set_xlabel('Class')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "meta_model = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "meta_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/MetaLearn/emgfan_7_c.pth'\n",
    "meta_model.load_state_dict(torch.load(save_dir_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Few Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, train_device, data, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data:\n",
    "            X = X.float().to(train_device)\n",
    "            y = y.long().to(train_device)\n",
    "            model = model.to(train_device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            total += y.size(0)\n",
    "            correct += (y_pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    return test_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FAN Fewshot Model with No Fine Tuning\")\n",
    "test_loss, test_acc = test_loop(meta_model, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Full Layer Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the model\n",
    "meta_model_full = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/MetaLearn/emgfan_7_c.pth'\n",
    "meta_model_full.load_state_dict(torch.load(save_dir_meta))\n",
    "meta_model_full.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(meta_model_full.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_loop(model, train_device, data, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "    # print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "    # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')) as prof:\n",
    "    #     with record_function(\"model_training\"):\n",
    "\n",
    "    for X, y in data:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X = X.float().to(train_device)\n",
    "            y = y.long().to(train_device)\n",
    "            model = model.to(train_device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total += y.size(0)\n",
    "            correct += (y_pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    # print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "    # print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "    # print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "    return train_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(meta_model_full, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"Fewshot Model Test On Full Layer FineTune\")\n",
    "test_loss, test_acc = test_loop(meta_model_full, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Last Layer Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the model\n",
    "meta_model_last = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/MetaLearn/emgfan_7_c.pth'\n",
    "meta_model_last.load_state_dict(torch.load(save_dir_meta))\n",
    "meta_model_last.to(device)\n",
    "optimizer = optim.Adam(meta_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in meta_model_last.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in meta_model_last.last.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "def is_layer_frozen(layer):\n",
    "    return all(not param.requires_grad for param in layer.parameters())\n",
    "print(\"fc1 is frozen:\", is_layer_frozen(meta_model_last.last))\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(meta_model_last, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"Fewshot Model Test with Last Layer FineTune\")\n",
    "test_loss, test_acc = test_loop(meta_model_last, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. TinyTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_tinytl = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/MetaLearn/emgfan_7_c.pth'\n",
    "meta_model_tinytl.load_state_dict(torch.load(save_dir_meta))\n",
    "meta_model_tinytl.to(device)\n",
    "optimizer = optim.Adam(meta_model_tinytl.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for name, param in meta_model_tinytl.named_parameters():\n",
    "    \n",
    "    if 'weight' in name:\n",
    "        param.requires_grad = False # Freeze the weights\n",
    "    elif 'bias' in name:\n",
    "        param.requires_grad = True # Unfreeze the bias\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(meta_model_tinytl, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"FewShot Model Test with TinyTL FineTune\")\n",
    "test_loss, test_acc = test_loop(meta_model_tinytl, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. EdgeEMGAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_ours = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/MetaLearn/emgfan_7_c.pth'\n",
    "meta_model_ours.load_state_dict(torch.load(save_dir_meta))\n",
    "meta_model_ours.to(device)\n",
    "optimizer = optim.Adam(meta_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in meta_model_ours.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in meta_model_ours.last.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in meta_model_ours.first.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# def is_layer_frozen(layer):\n",
    "#     return all(not param.requires_grad for param in layer.parameters())\n",
    "# print(\"Last Layer is Forzen:\", is_layer_frozen(kd_model_last.fc2))\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(meta_model_ours, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Test on Our Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(meta_model_ours, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 PreTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/PreTrain/7_Channels/emgfan_pretrain_15.pth'\n",
    "pretrain_model.load_state_dict(torch.load(save_dir_meta))\n",
    "pretrain_model.to(device)\n",
    "\n",
    "print(f\"PreTrain Model Test On No FineTune\")\n",
    "test_loss, test_acc = test_loop(pretrain_model, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Full Layer Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_full = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/PreTrain/7_Channels/emgfan_pretrain_15.pth'\n",
    "pretrain_model_full.load_state_dict(torch.load(save_dir_meta))\n",
    "pretrain_model_full.to(device)\n",
    "\n",
    "optimizer = optim.Adam(pretrain_model_full.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(pretrain_model_full, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"PreTrain Test on Full Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(pretrain_model_full, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Last Layer Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_last = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/PreTrain/7_Channels/emgfan_pretrain_15.pth'\n",
    "pretrain_model_last.load_state_dict(torch.load(save_dir_meta))\n",
    "pretrain_model_last.to(device)\n",
    "optimizer = optim.Adam(pretrain_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for param in pretrain_model_last.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in pretrain_model_last.last.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "def is_layer_frozen(layer):\n",
    "    return all(not param.requires_grad for param in layer.parameters())\n",
    "print(\"Last Layer is Forzen:\", is_layer_frozen(pretrain_model_last.last))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(pretrain_model_last, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "# tracker.stop()\n",
    "end_time = time()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"Test on Full Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(pretrain_model_last, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. TinyTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_tinytl = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/PreTrain/7_Channels/emgfan_pretrain_15.pth'\n",
    "pretrain_model_tinytl.load_state_dict(torch.load(save_dir_meta))\n",
    "pretrain_model_tinytl.to(device)\n",
    "optimizer = optim.Adam(pretrain_model_tinytl.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "for name, param in pretrain_model_tinytl.named_parameters():\n",
    "    \n",
    "    if 'weight' in name:\n",
    "        param.requires_grad = False # Freeze the weights\n",
    "    elif 'bias' in name:\n",
    "        param.requires_grad = True # Unfreeze the bias\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(pretrain_model_tinytl, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"Pretrain Model Test with TinyTL FineTune\")\n",
    "test_loss, test_acc = test_loop(pretrain_model_tinytl, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. EdgeEMGAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_ours = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/PreTrain/7_Channels/emgfan_pretrain_15.pth'\n",
    "pretrain_model_ours.load_state_dict(torch.load(save_dir_meta))\n",
    "pretrain_model_ours.to(device)\n",
    "optimizer = optim.Adam(pretrain_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in pretrain_model_ours.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in pretrain_model_ours.last.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in pretrain_model_ours.first.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# def is_layer_frozen(layer):\n",
    "#     return all(not param.requires_grad for param in layer.parameters())\n",
    "# print(\"Last Layer is Forzen:\", is_layer_frozen(kd_model_last.fc2))\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(pretrain_model_ours, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Test on Our Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(pretrain_model_ours, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "kd_model.load_state_dict(torch.load(save_dir_meta))\n",
    "kd_model.to(device)\n",
    "\n",
    "print(f\"PreTrain Model Test On No FineTune\")\n",
    "test_loss, test_acc = test_loop(kd_model, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Full Layer Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_full = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "kd_model_full.load_state_dict(torch.load(save_dir_meta))\n",
    "kd_model_full.to(device)\n",
    "\n",
    "optimizer = optim.Adam(kd_model_full.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(kd_model_full, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "print(f\"KD Test on Full Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(kd_model_full, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Last Layer Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_last = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "kd_model_last.load_state_dict(torch.load(save_dir_meta))\n",
    "kd_model_last.to(device)\n",
    "optimizer = optim.Adam(kd_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in kd_model_last.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in kd_model_last.last.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "def is_layer_frozen(layer):\n",
    "    return all(not param.requires_grad for param in layer.parameters())\n",
    "print(\"Last Layer is Forzen:\", is_layer_frozen(kd_model_last.last))\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(kd_model_last, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Test on Last Layer Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(kd_model_last, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. TinyTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_tinytl = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "kd_model_tinytl.load_state_dict(torch.load(save_dir_meta))\n",
    "kd_model_tinytl.to(device)\n",
    "optimizer = optim.Adam(kd_model_tinytl.parameters(), lr=learning_rate)\n",
    "\n",
    "for name, param in kd_model_tinytl.named_parameters():\n",
    "    \n",
    "    if 'weight' in name:\n",
    "        param.requires_grad = False # Freeze the weights\n",
    "    elif 'bias' in name:\n",
    "        param.requires_grad = True # Unfreeze the bias\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(kd_model_tinytl, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Model Test with TinyTL FineTune\")\n",
    "test_loss, test_acc = test_loop(kd_model_tinytl, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. EdgeEMGAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_ours = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "kd_model_ours.load_state_dict(torch.load(save_dir_meta))\n",
    "kd_model_ours.to(device)\n",
    "optimizer = optim.Adam(kd_model_last.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in kd_model_ours.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in kd_model_ours.last.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in kd_model_ours.first.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# def is_layer_frozen(layer):\n",
    "#     return all(not param.requires_grad for param in layer.parameters())\n",
    "# print(\"Last Layer is Forzen:\", is_layer_frozen(kd_model_last.fc2))\n",
    "\n",
    "\n",
    "print(f\"FineTune on {device}\")\n",
    "train_accuracy_per_epoch = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = fine_tune_loop(kd_model_ours, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Test on Our Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(kd_model_ours, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max\n",
    "\n",
    "def plot_weight_distribution(model, bitwidth=32):\n",
    "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
    "    if bitwidth <= 8:\n",
    "        qmin, qmax = get_quantized_range(bitwidth)\n",
    "        bins = np.arange(qmin, qmax + 2)\n",
    "        align = 'left'\n",
    "    else:\n",
    "        bins = 256\n",
    "        align = 'mid'\n",
    "    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n",
    "    axes = axes.ravel()\n",
    "    plot_index = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            ax = axes[plot_index]\n",
    "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n",
    "                    align=align, color = 'blue', alpha = 0.5,\n",
    "                    edgecolor='black' if bitwidth <= 4 else None)\n",
    "            if bitwidth <= 4:\n",
    "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel('density')\n",
    "            plot_index += 1\n",
    "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.925)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def get_quantization_scale_for_weight(weight, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor of weight\n",
    "    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [integer] quantization bit width\n",
    "    :return:\n",
    "        [floating scalar] scale\n",
    "    \"\"\"\n",
    "    # we just assume values in weight are symmetric\n",
    "    # we also always make zero_point 0 for weight\n",
    "    fp_max = max(weight.abs().max().item(), 5e-7)\n",
    "    _, quantized_max = get_quantized_range(bitwidth)\n",
    "    return fp_max / quantized_max\n",
    "\n",
    "\n",
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    linear quantization for single fp_tensor\n",
    "      from\n",
    "        fp_tensor = (quantized_tensor - zero_point) * scale\n",
    "      we have,\n",
    "        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
    "    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n",
    "    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n",
    "    :return:\n",
    "        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n",
    "    \"\"\"\n",
    "    assert(fp_tensor.dtype == torch.float)\n",
    "    assert(isinstance(scale, float) or\n",
    "           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
    "    assert(isinstance(zero_point, int) or\n",
    "           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 1: scale the fp_tensor\n",
    "    scaled_tensor = fp_tensor / scale\n",
    "    # Step 2: round the floating value to integer value\n",
    "    rounded_tensor = torch.round(scaled_tensor)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    rounded_tensor = rounded_tensor.to(dtype)\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 3: shift the rounded_tensor to make zero_point 0\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
    "    return quantized_tensor\n",
    "\n",
    "\n",
    "def linear_quantize_weight_per_channel(tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    linear quantization for weight tensor\n",
    "        using different scales and zero_points for different output channels\n",
    "    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [torch.(cuda.)Tensor] scale tensor\n",
    "        [int] zero point (which is always 0)\n",
    "    \"\"\"\n",
    "    dim_output_channels = 0\n",
    "    num_output_channels = tensor.shape[dim_output_channels] # Select Output Channel e.g. 10\n",
    "    scale = torch.zeros(num_output_channels, device=tensor.device)\n",
    "    for oc in range(num_output_channels): # Loop through each channel\n",
    "        _subtensor = tensor.select(dim_output_channels, oc) # Select each channel\n",
    "        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth) # Get the scale for each channel\n",
    "        scale[oc] = _scale\n",
    "    scale_shape = [1] * tensor.dim() # Expand the dims\n",
    "    scale_shape[dim_output_channels] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n",
    "    return quantized_tensor, scale, 0\n",
    "\n",
    "\n",
    "\n",
    "def plot_weight_distribution_new(model, bitwidth=32):\n",
    "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
    "    if bitwidth <= 8:\n",
    "        qmin, qmax = get_quantized_range(bitwidth)\n",
    "        bins = np.arange(qmin, qmax + 2)\n",
    "        align = 'left'\n",
    "    else:\n",
    "        bins = 256\n",
    "        align = 'mid'\n",
    "    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n",
    "    axes = axes.ravel()\n",
    "    plot_index = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            ax = axes[plot_index]\n",
    "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n",
    "                    align=align, color = 'blue', alpha = 0.5,\n",
    "                    edgecolor='black' if bitwidth <= 4 else None)\n",
    "            if bitwidth <= 4:\n",
    "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel('density')\n",
    "            plot_index += 1\n",
    "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.925)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weight_distribution(model, bitwidth=32):\n",
    "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
    "    if bitwidth <= 8:\n",
    "        qmin, qmax = get_quantized_range(bitwidth)\n",
    "        bins = np.arange(qmin, qmax + 2)\n",
    "        align = 'left'\n",
    "    else:\n",
    "        bins = 256\n",
    "        align = 'mid'\n",
    "    \n",
    "    # Change the number of subplots to 5\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(10, 6))  # 3 rows, 2 columns\n",
    "    axes = axes.ravel()\n",
    "    plot_index = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 1 and plot_index < 5:  # Limit to 5 plots\n",
    "            ax = axes[plot_index]\n",
    "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n",
    "                    align=align, color='blue', alpha=0.5,\n",
    "                    edgecolor='black' if bitwidth <= 4 else None)\n",
    "            if bitwidth <= 4:\n",
    "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max + 1))\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel('density')\n",
    "            plot_index += 1\n",
    "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth - 1} bits)')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.925)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_last_train = torch.save(kd_model_last.state_dict(), '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/On-Device/edge_last_train.pth')\n",
    "\n",
    "def recover_model(model, save_dir):\n",
    "    model.load_state_dict(torch.load(save_dir))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_distribution(kd_model_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitwidth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_bit_quantization(model, bitwidth=8):\n",
    "    \"\"\"\n",
    "    Quantizes the model parameters except for the specified layers.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be quantized.\n",
    "        bitwidth: The bit width for quantization (default is 8).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (quantized_model, original_model)\n",
    "    \"\"\"\n",
    "    # Create a copy of the original model\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model_2 = copy.deepcopy(model)\n",
    "    scales = {}\n",
    "    \n",
    "    # Calculate scale factors for each parameter\n",
    "    for name, param in quantized_model.named_parameters():\n",
    "        max_scale_factor = param.max()\n",
    "        scales[name] = max_scale_factor\n",
    "\n",
    "    # Apply quantization to all parameters except specified layers\n",
    "    for name, param in quantized_model.named_parameters():\n",
    "        if name not in ['first.weight', 'first.bias', 'last.weight', 'last.bias']:\n",
    "            param.data = param.data.sign()  # Quantization step\n",
    "\n",
    "    for name, param in quantized_model_2.named_parameters():\n",
    "        if name not in ['first.weight', 'first.bias', 'last.weight', 'last.bias']:\n",
    "            param.data = param.data.sign() * scales[name]  # Quantization step\n",
    "\n",
    "    return quantized_model, quantized_model_2, model  # Return both quantized and original models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_last_model_one, edge_last_model_one_scale, model = one_bit_quantization(kd_model_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def peek_linear_quantization():\n",
    "    for name, param in edge_last_model_one.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            quantized_param, scale, zero_point = \\\n",
    "                linear_quantize_weight_per_channel(param, bitwidth)\n",
    "            param.copy_(quantized_param)\n",
    "    plot_weight_distribution(edge_last_model_one, bitwidth)\n",
    "        # recover_model()\n",
    "\n",
    "peek_linear_quantization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTQ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_loop(edge_last_model_one, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test_loop(edge_last_model_one_scale, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_last_model_one.FAN.input_linear_p.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(tensor):\n",
    "    return tensor.sign()\n",
    "\n",
    "class BinarizeLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BinarizeLinear, self).__init__(in_features, out_features)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input * weight\n",
    "        \n",
    "        # binarize input\n",
    "        input.data = binarize(input.data) # Binarize the tensor\n",
    "\n",
    "        # binarize weight\n",
    "        if not hasattr(self.weight, 'org'):\n",
    "            self.weight.org = self.weight.data.clone()\n",
    "            \n",
    "        self.weight.data = binarize(self.weight.org)\n",
    "\n",
    "        res = nn.functional.linear(input, self.weight)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class BinarizeConv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(BinarizeConv, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                                           padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input * weight\n",
    "        \n",
    "        # binarize input\n",
    "        input.data = binarize(input.data) # Binarize the tensor\n",
    "\n",
    "        # binarize weight\n",
    "        if not hasattr(self.weight, 'org'):\n",
    "            self.weight.org = self.weight.data.clone()\n",
    "            \n",
    "        self.weight.data = binarize(self.weight.org)\n",
    "\n",
    "        res = nn.functional.conv2d(input, self.weight)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FANLayer(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, p_ratio=0.25):\n",
    "        super(FANLayer, self).__init__()\n",
    "        \n",
    "        # Ensure the p_ratio is within a valid range\n",
    "        assert 0 < p_ratio < 0.5, \"p_ratio must be between 0 and 0.5\"\n",
    "        \n",
    "        self.p_ratio = p_ratio\n",
    "        p_output_dim = int(output_dim * self.p_ratio)\n",
    "        g_output_dim = output_dim - p_output_dim * 2  # Account for cosine and sine terms\n",
    "\n",
    "        # Linear transformation for the p component (for cosine and sine parts)\n",
    "        #self.input_linear_p = BinarizeLinear(input_dim, p_output_dim)\n",
    "        self.input_linear_p = nn.Linear(input_dim, p_output_dim)\n",
    "        \n",
    "        # Linear transformation for the g component\n",
    "        #self.input_linear_g = BinarizeLinear(input_dim, g_output_dim)\n",
    "        self.input_linear_g = nn.Linear(input_dim, g_output_dim)\n",
    "\n",
    "        #self.activation = nn.Hardtanh()\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, output_dim), after applying the FAN layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply the linear transformation followed by the activation for the g component\n",
    "\n",
    "        g = self.activation(self.input_linear_g(src))\n",
    "        \n",
    "        # Apply the linear transformation for the p component\n",
    "        p = self.input_linear_p(src)\n",
    "\n",
    "        # Concatenate cos(p), sin(p), and activated g along the last dimension\n",
    "        output = torch.cat((torch.cos(p), torch.sin(p), g), dim=-1)\n",
    "\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGFANNew(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=7, similarparameter=False):\n",
    "        super(EMGFANNew, self).__init__()\n",
    "        self.similarparameter = similarparameter\n",
    "        self.out_gesture = output_dim\n",
    "        self.in_channel = input_dim\n",
    "\n",
    "        self.first = nn.Conv2d(self.in_channel, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.htanh1 = nn.ReLU()\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        #self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "\n",
    "        self.conv2 = BinarizeConv(32, 32, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.htanh2 = nn.ReLU()\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "\n",
    "        # self.conv3 = nn.Conv2d(32, 32, kernel_size=1)\n",
    "        # self.bn = nn.BatchNorm2d(32)\n",
    "        # self.act = nn.ReLU()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.scalar = lambda x: x*4//3 if self.similarparameter else x\n",
    "        self.FAN = FANLayer(576, self.scalar(256))\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        #self.htanh3 = nn.ReLU()\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "\n",
    "        self.last = nn.Linear(256, self.out_gesture)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.htanh1(x)\n",
    "\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.htanh2(x)\n",
    "\n",
    "        \n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.bn(x)\n",
    "        # x = self.act(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.FAN(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.htanh3(x)\n",
    "\n",
    "        x = self.last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAT_fine_tune_loop(model, train_device, data, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "    # print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n",
    "    # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')) as prof:\n",
    "    #     with record_function(\"model_training\"):\n",
    "\n",
    "    for X, y in data:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X = X.float().to(train_device)\n",
    "            y = y.long().to(train_device)\n",
    "            model = model.to(train_device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p, 'org'):\n",
    "                    p.data.copy_(p.org) # rest      \n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p, 'org'):\n",
    "                    p.org.copy_(p.data.clamp_(-1, 1))\n",
    "        \n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total += y.size(0)\n",
    "            correct += (y_pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    # print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "    # print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "    # print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "    return train_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_bit_qat = EMGFANNew(1, number_gestures, similarparameter=False)\n",
    "save_dir_meta = '/mnt/d/AI-Workspace/sEMGClassification/AdaptiveModel/code/models/model_weights/EMGFAN/KDMetaLearn/emgfankd_7_c.pth'\n",
    "model_one_bit_qat .load_state_dict(torch.load(save_dir_meta))\n",
    "model_one_bit_qat .to(device)\n",
    "optimizer = optim.Adam(model_one_bit_qat.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_bit_qat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss, train_acc = QAT_fine_tune_loop(model_one_bit_qat, device, train_dataloader, criterion, optimizer)\n",
    "    train_accuracy_per_epoch.append(train_acc)\n",
    "\n",
    "\n",
    "end_time = time()\n",
    "# tracker.stop()\n",
    "AVG_TRAIN_ACC = np.mean(train_accuracy_per_epoch)\n",
    "print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "print(f\"The Average FineTune Accuracy: {AVG_TRAIN_ACC*100:.4f}%\")\n",
    "print(f'The Last FineTune Accuracy {train_acc*100:.4f}%')\n",
    "\n",
    "\n",
    "print(f\"KD Test on Last Layer Fine Tune {device}\")\n",
    "test_loss, test_acc = test_loop(model_one_bit_qat, device, test_dataloader, criterion)\n",
    "print(f'Test accuracy {test_acc*100:.4f}%')\n",
    "print(f'Training Time is {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def peek_linear_quantization():\n",
    "    for name, param in model_one_bit_qat.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            quantized_param, scale, zero_point = \\\n",
    "                linear_quantize_weight_per_channel(param, bitwidth)\n",
    "            param.copy_(quantized_param)\n",
    "    plot_weight_distribution(model_one_bit_qat, bitwidth)\n",
    "        # recover_model()\n",
    "\n",
    "peek_linear_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_bit_qat.conv2.weight.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
